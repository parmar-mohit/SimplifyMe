{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511fa5d0-c14a-4c6c-bf69-4a2b54c07aaf",
   "metadata": {},
   "source": [
    "# Comparision CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fbe4829-ef5d-4d0b-8064-da2e8d7edd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from language_tool_python import LanguageTool\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29bf707c-6f6e-4617-8829-30c430b9f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_model_path = \"../iter_trained_model\"\n",
    "text_model_path = \"facebook/bart-large-cnn\"\n",
    "tokenizer_path = \"facebook/bart-large-cnn\"\n",
    "\n",
    "directory = \"../Test Paper Data/\" \n",
    "output_file = \"../comparision_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91f7afac-0c78-4ddd-baa7-570639196f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_section_headers(text):\n",
    "    # Define the regular expression pattern to match sequences like \"1. xy\"\n",
    "    section_patterns = [\n",
    "        # Digits\n",
    "        r'\\b\\d+\\.\\s+\\w+\\b',\n",
    "        # Roman Numerals\n",
    "        r'\\b[IVXLCDM]+\\.\\s+\\w+\\b'\n",
    "    ]\n",
    "\n",
    "    # Combine patterns into a single regex pattern\n",
    "    pattern = '|'.join(section_patterns)\n",
    "\n",
    "    # Use re.sub to replace matched sequences with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def exclude_header_footer(text):\n",
    "    # Split text into sentences using regex\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    \n",
    "    # No of sentences to exclude in header\n",
    "    num_sentences_to_exclude_header = int( len(sentences) * 0.05 )\n",
    "    # No of sentences to exclude in footer\n",
    "    num_sentences_to_exclude_footer = int(len(sentences) * 0.1 )\n",
    "    \n",
    "    # Exclude the header and footer  of page\n",
    "    excluded_sentences = sentences[num_sentences_to_exclude_header:-num_sentences_to_exclude_footer]\n",
    "    \n",
    "    # Rejoin the remaining sentences\n",
    "    remaining_text = ' '.join(excluded_sentences)\n",
    "    \n",
    "    return remaining_text\n",
    "\n",
    "def remove_citations(text):\n",
    "    # Define patterns for common citation formats\n",
    "    citation_patterns = [\n",
    "        # Match citations like \"[1]\", \"[12]\", \"[123]\", etc.\n",
    "        r'\\[\\d+\\]',\n",
    "        \n",
    "        # Match citations like \"(Author, Year)\", \"(Author et al., Year)\", \"(Author Year)\", etc.\n",
    "        r'\\(\\w+(?: et al.)?, \\d{4}\\)',\n",
    "        \n",
    "        # Match citations like \"[Author et al., Year]\", \"[Author, Year]\", etc.\n",
    "        r'\\[\\w+(?: et al.)?, \\d{4}\\]'\n",
    "    ]\n",
    "    \n",
    "    # Combine patterns into a single regex pattern\n",
    "    combined_pattern = '|'.join(citation_patterns)\n",
    "    \n",
    "    # Remove citations from the text using the regex pattern\n",
    "    cleaned_text = re.sub(combined_pattern, '', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def replace_multiple_whitespace(text):\n",
    "    # Define the regular expression pattern to match multiple whitespace characters\n",
    "    pattern = r'\\s+'\n",
    "    \n",
    "    # Use re.sub to replace multiple whitespace characters with a single whitespace character\n",
    "    cleaned_text = re.sub(pattern, ' ', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def fix_grammar(text):\n",
    "    tool = LanguageTool('en-US')\n",
    "\n",
    "    # Check for grammatical errors\n",
    "    matches = tool.check(text)\n",
    "    # Fix grammatical errors\n",
    "    corrected_text = tool.correct(text)\n",
    "\n",
    "    return corrected_text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # replace new line character with space\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "\n",
    "    # removing extra whitespaces\n",
    "    text = replace_multiple_whitespace(text)\n",
    "\n",
    "    # remove citations\n",
    "    text = remove_citations(text)\n",
    "\n",
    "    #remove section heading\n",
    "    text = remove_section_headers(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_paper_content(pdf_file_path):\n",
    "    with open(pdf_file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        full_text = []\n",
    "\n",
    "        for page_number in range(num_pages):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            page_content = page.extract_text()\n",
    "            cleaned_content = preprocess_text(page_content)\n",
    "            cleaned_content = exclude_header_footer(cleaned_content)\n",
    "            full_text.append(cleaned_content)\n",
    "\n",
    "        final_text = '\\n'.join(full_text)\n",
    "\n",
    "        return final_text\n",
    "\n",
    "def generate_summary_paper(text, min_summary_length=128, max_summary_length=1024, overlap_percentage=35):\n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    text = fix_grammar(text)\n",
    "    \n",
    "    model = BartForConditionalGeneration.from_pretrained(paper_model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "    tokenized_text = tokenizer.encode(text, return_tensors=\"pt\", truncation=False)\n",
    "    # Calculate overlap size\n",
    "    max_tokens = 1024\n",
    "    overlap_size = int(max_tokens * overlap_percentage / 100)\n",
    "\n",
    "    # Generate summaries with overlapping chunks\n",
    "    summaries = []\n",
    "    for i in range(0, tokenized_text.size(1) - max_tokens + 1, max_tokens - overlap_size):\n",
    "        start = i\n",
    "        end = min(i + max_tokens, tokenized_text.size(1))\n",
    "        chunk = tokenized_text[:, start:end]\n",
    "        summary_ids = model.generate(chunk, min_length=min_summary_length, max_length=max_summary_length, num_beams=10, early_stopping=True, length_penalty=0.8, repetition_penalty=1.5)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # Concatenate summaries to create the final summary\n",
    "    final_summary = \" \".join(summaries)\n",
    "    final_summary = final_summary.replace(\"\\n\",\" \")\n",
    "    return fix_grammar(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffde5f9d-6e3f-4256-864a-f541028aac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    paper_content = get_paper_content(file_path)\n",
    "    generate_summary_paper(paper_content)\n",
    "\n",
    "def process_files_in_directory(directory):\n",
    "    results = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        start_time = time.time()\n",
    "        process_file(file_path)\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        results.append((filename, processing_time))\n",
    "        print(filename + \" Processed\")\n",
    "        save_results_to_csv(results, output_file)\n",
    "    return results\n",
    "\n",
    "def save_results_to_csv(results, output_file):\n",
    "    df = pd.DataFrame(results, columns=['Filename', 'Processing Time (s)'])\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f5411-0fd8-4b14-9676-9bf5d9ae6d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files in directory: ../Test Paper Data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7979 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10005108.pdf Processed\n",
      "Results saved to ../comparision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5238 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10018187.pdf Processed\n",
      "Results saved to ../comparision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11011 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10032547.pdf Processed\n",
      "Results saved to ../comparision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11545 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10044683.pdf Processed\n",
      "Results saved to ../comparision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7761 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10054384.pdf Processed\n",
      "Results saved to ../comparision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18643 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(directory):\n",
    "    print(f\"Processing files in directory: {directory}\")\n",
    "    results = process_files_in_directory(directory)\n",
    "else:\n",
    "    print(f\"Error: {directory} is not a valid directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
