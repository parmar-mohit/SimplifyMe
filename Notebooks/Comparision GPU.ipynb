{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7776295,"sourceType":"datasetVersion","datasetId":4550071},{"sourceId":14244,"sourceType":"modelInstanceVersion","modelInstanceId":11799}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Comparision GPU","metadata":{"id":"S54ZUJZwAIWU"}},{"cell_type":"code","source":"!pip install pandas torch PyPDF2 language-tool-python transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPVegU3-DiOu","outputId":"597e8e8c-3fc3-48d1-b0d0-e6dffe70aa90","execution":{"iopub.status.busy":"2024-03-10T09:19:50.700970Z","iopub.execute_input":"2024-03-10T09:19:50.701382Z","iopub.status.idle":"2024-03-10T09:20:05.281244Z","shell.execute_reply.started":"2024-03-10T09:19:50.701328Z","shell.execute_reply":"2024-03-10T09:20:05.280237Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.1.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nCollecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nCollecting language-tool-python\n  Downloading language_tool_python-2.7.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.1)\nRequirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from language-tool-python) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from language-tool-python) (4.66.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->language-tool-python) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->language-tool-python) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->language-tool-python) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->language-tool-python) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\nInstalling collected packages: PyPDF2, language-tool-python\nSuccessfully installed PyPDF2-3.0.1 language-tool-python-2.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport time\nimport PyPDF2\nimport re\nimport torch\nimport gc\nimport threading\nfrom language_tool_python import LanguageTool\nfrom transformers import AutoTokenizer, BartForConditionalGeneration","metadata":{"id":"SVfGlR-wACEy","execution":{"iopub.status.busy":"2024-03-10T09:20:05.283224Z","iopub.execute_input":"2024-03-10T09:20:05.283542Z","iopub.status.idle":"2024-03-10T09:20:12.995441Z","shell.execute_reply.started":"2024-03-10T09:20:05.283515Z","shell.execute_reply":"2024-03-10T09:20:12.994410Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"paper_model_path = \"/kaggle/input/iter-papers-trained/other/gen1/1\"\ntokenizer_path = \"facebook/bart-large-cnn\"\n\ndirectory = \"/kaggle/input/test-paper-data\"\noutput_file = \"/kaggle/working/comparision_results_gpu.csv\"\n\ndevice = \"cuda\"\nprint(torch.cuda.is_available())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zW6IOGERAO-X","outputId":"5462f6c6-169d-4567-a679-d8e2d705af2f","execution":{"iopub.status.busy":"2024-03-10T09:20:12.996772Z","iopub.execute_input":"2024-03-10T09:20:12.997242Z","iopub.status.idle":"2024-03-10T09:20:13.031594Z","shell.execute_reply.started":"2024-03-10T09:20:12.997213Z","shell.execute_reply":"2024-03-10T09:20:13.030197Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}]},{"cell_type":"code","source":"def remove_section_headers(text):\n    # Define the regular expression pattern to match sequences like \"1. xy\"\n    section_patterns = [\n        # Digits\n        r'\\b\\d+\\.\\s+\\w+\\b',\n        # Roman Numerals\n        r'\\b[IVXLCDM]+\\.\\s+\\w+\\b'\n    ]\n\n    # Combine patterns into a single regex pattern\n    pattern = '|'.join(section_patterns)\n\n    # Use re.sub to replace matched sequences with an empty string\n    cleaned_text = re.sub(pattern, '', text)\n\n    return cleaned_text\n\ndef exclude_header_footer(text):\n    # Split text into sentences using regex\n    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n\n    # No of sentences to exclude in header\n    num_sentences_to_exclude_header = int( len(sentences) * 0.05 )\n    # No of sentences to exclude in footer\n    num_sentences_to_exclude_footer = int(len(sentences) * 0.1 )\n\n    # Exclude the header and footer  of page\n    excluded_sentences = sentences[num_sentences_to_exclude_header:-num_sentences_to_exclude_footer]\n\n    # Rejoin the remaining sentences\n    remaining_text = ' '.join(excluded_sentences)\n\n    return remaining_text\n\ndef remove_citations(text):\n    # Define patterns for common citation formats\n    citation_patterns = [\n        # Match citations like \"[1]\", \"[12]\", \"[123]\", etc.\n        r'\\[\\d+\\]',\n\n        # Match citations like \"(Author, Year)\", \"(Author et al., Year)\", \"(Author Year)\", etc.\n        r'\\(\\w+(?: et al.)?, \\d{4}\\)',\n\n        # Match citations like \"[Author et al., Year]\", \"[Author, Year]\", etc.\n        r'\\[\\w+(?: et al.)?, \\d{4}\\]'\n    ]\n\n    # Combine patterns into a single regex pattern\n    combined_pattern = '|'.join(citation_patterns)\n\n    # Remove citations from the text using the regex pattern\n    cleaned_text = re.sub(combined_pattern, '', text)\n\n    return cleaned_text\n\ndef replace_multiple_whitespace(text):\n    # Define the regular expression pattern to match multiple whitespace characters\n    pattern = r'\\s+'\n\n    # Use re.sub to replace multiple whitespace characters with a single whitespace character\n    cleaned_text = re.sub(pattern, ' ', text)\n\n    return cleaned_text\n\ndef fix_grammar(text):\n    tool = LanguageTool('en-US')\n\n    # Check for grammatical errors\n    matches = tool.check(text)\n    # Fix grammatical errors\n    corrected_text = tool.correct(text)\n\n    return corrected_text\n\ndef preprocess_text(text):\n    # replace new line character with space\n    text = text.replace(\"\\n\",\" \")\n\n    # removing extra whitespaces\n    text = replace_multiple_whitespace(text)\n\n    # remove citations\n    text = remove_citations(text)\n\n    #remove section heading\n    text = remove_section_headers(text)\n\n    return text\n\ndef get_paper_content(pdf_file_path):\n    with open(pdf_file_path, 'rb') as file:\n        pdf_reader = PyPDF2.PdfReader(file)\n        num_pages = len(pdf_reader.pages)\n        full_text = []\n\n        for page_number in range(num_pages):\n            page = pdf_reader.pages[page_number]\n            page_content = page.extract_text()\n            cleaned_content = preprocess_text(page_content)\n            cleaned_content = exclude_header_footer(cleaned_content)\n            full_text.append(cleaned_content)\n\n        final_text = '\\n'.join(full_text)\n\n        file.close()\n\n        return final_text\n\ndef generate_summary_paper(text,model,tokenizer, min_summary_length=128, max_summary_length=1024, overlap_percentage=35):\n    text = preprocess_text(text)\n    text = fix_grammar(text)\n\n    tokenized_text = tokenizer.encode(text, return_tensors=\"pt\", truncation=False).to(device)\n    # Calculate overlap size\n    max_tokens = 1024\n    overlap_size = int(max_tokens * overlap_percentage / 100)\n\n    # Generate summaries with overlapping chunks\n    summaries = []\n    summary_tokens = 0\n    for i in range(0, tokenized_text.size(1) - max_tokens + 1, max_tokens - overlap_size):\n        start = i\n        end = min(i + max_tokens, tokenized_text.size(1))\n        chunk = tokenized_text[:, start:end]\n        summary_ids = model.generate(chunk, min_length=min_summary_length, max_length=max_summary_length, num_beams=10, early_stopping=True, length_penalty=0.8, repetition_penalty=1.5)\n        summary_ids.to(device)\n        summary_tokens += summary_ids.size(1)\n        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n        summaries.append(summary)\n\n    # Concatenate summaries to create the final summary\n    final_summary = \" \".join(summaries)\n    final_summary = final_summary.replace(\"\\n\",\" \")\n    final_summary = fix_grammar(final_summary)\n    result = (final_summary,tokenized_text.size(1),summary_tokens)\n\n    del final_summary\n    del summaries\n    del summary_ids\n    del summary_tokens\n    del summary\n    del chunk\n    del text\n    del tokenized_text\n    gc.collect()\n\n    return result","metadata":{"id":"-A5TsHlqAW-N","execution":{"iopub.status.busy":"2024-03-10T09:20:13.033875Z","iopub.execute_input":"2024-03-10T09:20:13.034197Z","iopub.status.idle":"2024-03-10T09:20:13.055418Z","shell.execute_reply.started":"2024-03-10T09:20:13.034162Z","shell.execute_reply":"2024-03-10T09:20:13.054277Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def count_words(text):\n    words = text.split()\n    return len(words)","metadata":{"id":"xJCO6EAOTb_9","execution":{"iopub.status.busy":"2024-03-10T09:20:13.056667Z","iopub.execute_input":"2024-03-10T09:20:13.057013Z","iopub.status.idle":"2024-03-10T09:20:13.070420Z","shell.execute_reply.started":"2024-03-10T09:20:13.056981Z","shell.execute_reply":"2024-03-10T09:20:13.069602Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def process_file(file_path,model,tokenizer):\n    start_time = time.time()\n    paper_content = get_paper_content(file_path)\n    result = generate_summary_paper(paper_content,model,tokenizer)\n    end_time = time.time()\n    processing_time = end_time - start_time\n    res =  {\"filename\":[os.path.basename(file_path)],\"gpu_time\":[processing_time],\"paper_words\":[count_words(paper_content)],\"summary_words\":[count_words(result[0])],\"paper_tokens\":[result[1]],\"summary_tokens\":[result[2]]}\n\n    del paper_content\n    del processing_time\n    del result\n    gc.collect()\n\n    print(\"{} file processed successfully\".format(os.path.basename(file_path)))\n    return res\n\ndef process_files_in_directory(df,directory,model,tokenizer):\n    files = [os.path.join(directory, file) for file in os.listdir(directory)]\n    for file in files:\n        if not df[\"filename\"].str.contains(os.path.basename(file)).any():\n          result = process_file(file,model,tokenizer)\n          df = pd.concat([df,pd.DataFrame(result)],ignore_index=True)\n          df.to_csv(output_file,index=False)\n          print(\"Total {} files Processed\".format(len(df) ) )\n\n          # Triggering Garbage Collector\n          gc.collect()","metadata":{"id":"C7JNAxuAAahY","execution":{"iopub.status.busy":"2024-03-10T09:20:13.071607Z","iopub.execute_input":"2024-03-10T09:20:13.072328Z","iopub.status.idle":"2024-03-10T09:20:13.083589Z","shell.execute_reply.started":"2024-03-10T09:20:13.072296Z","shell.execute_reply":"2024-03-10T09:20:13.082585Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists(output_file):\n    df = pd.DataFrame({'filename': [], 'gpu_time': [], \"paper_words\":[], \"summary_words\":[], \"paper_tokens\":[], \"summary_tokens\":[]})\n    df.to_csv(output_file,index=False)\n\ndf = pd.read_csv(output_file)","metadata":{"id":"8HtQdEGzAcd6","execution":{"iopub.status.busy":"2024-03-10T09:20:13.084655Z","iopub.execute_input":"2024-03-10T09:20:13.084910Z","iopub.status.idle":"2024-03-10T09:20:13.103976Z","shell.execute_reply.started":"2024-03-10T09:20:13.084887Z","shell.execute_reply":"2024-03-10T09:20:13.103124Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if os.path.isdir(directory):\n    print(f\"Processing files in directory: {directory}\")\n    model = BartForConditionalGeneration.from_pretrained(paper_model_path).to(device)\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,device=device)\n    process_files_in_directory(df,directory,model,tokenizer)\nelse:\n    print(f\"Error: {directory} is not a valid directory.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2HCSEu3-Aecl","outputId":"35453506-ad03-4ec8-c540-7e1287e759df","execution":{"iopub.status.busy":"2024-03-10T09:20:13.104998Z","iopub.execute_input":"2024-03-10T09:20:13.105279Z","iopub.status.idle":"2024-03-10T09:32:25.611800Z","shell.execute_reply.started":"2024-03-10T09:20:13.105257Z","shell.execute_reply":"2024-03-10T09:32:25.610732Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Processing files in directory: /kaggle/input/test-paper-data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49c4c6f31c4b42d080c68a8ebafaf21b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c86e00150554022861770cb7b353dd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cde936c3c024fbf9209c8478c253b5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7ec81e0622848489bb60655aca1411a"}},"metadata":{}},{"name":"stderr","text":"Downloading LanguageTool 5.7: 100%|██████████| 225M/225M [00:04<00:00, 52.9MB/s] \nToken indices sequence length is longer than the specified maximum sequence length for this model (9665 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"10113620.pdf file processed successfully\nTotal 95 files Processed\n8606919.pdf file processed successfully\nTotal 96 files Processed\n9745159.pdf file processed successfully\nTotal 97 files Processed\n9107114.pdf file processed successfully\nTotal 98 files Processed\n9328413.pdf file processed successfully\nTotal 99 files Processed\n9490211.pdf file processed successfully\nTotal 100 files Processed\n","output_type":"stream"}]}]}